{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as path\n",
    "import glob\n",
    "\n",
    "from csv import (writer, DictWriter)\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from drain3 import TemplateMiner\n",
    "from drain3.template_miner_config import TemplateMinerConfig\n",
    "\n",
    "from spellpy import spell\n",
    "from fastai.text.all import Numericalize\n",
    "\n",
    "import fasttext\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import (auc, roc_curve, average_precision_score, precision_recall_curve, f1_score, accuracy_score,\n",
    "                            recall_score, precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.getcwd()\n",
    "\n",
    "data_dir = 'data'\n",
    "log_file_name = 'bgl_2k.log'\n",
    "\n",
    "config_dir = 'config'\n",
    "\n",
    "input_dir = 'input'\n",
    "spell_output_dir = 'output'\n",
    "\n",
    "spell_input_dir = 'input/spell'\n",
    "spell_output_dir = 'output/spell'\n",
    "\n",
    "drain_input_dir = 'input/drain'\n",
    "drain_output_dir = 'output/drain'\n",
    "\n",
    "WINDOW_SIZE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert BGL.log file to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_csv_file_name = 'bgl_2k.csv'\n",
    "log_csv_file = log_file = path.abspath(path.join(project_root, input_dir, log_csv_file_name))\n",
    "\n",
    "if os.path.exists(log_csv_file):\n",
    "    os.remove(log_csv_file)\n",
    "\n",
    "log_file = path.abspath(path.join(project_root, data_dir, log_file_name))\n",
    "logs = open(log_file, 'r')\n",
    "\n",
    "with open(log_csv_file, 'a') as log_csv_file_obj:\n",
    "    log_csv_writer_obj = writer(log_csv_file_obj)\n",
    "    for line in logs:\n",
    "        split_data = line.rstrip('\\n').split(' ')\n",
    "        split_data[9] = ' '.join(split_data[9:])\n",
    "        log_csv_writer_obj.writerow(split_data[0:10])\n",
    "    log_csv_file_obj.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly Type</th>\n",
       "      <th>Timestamp (ms)</th>\n",
       "      <th>Date</th>\n",
       "      <th>Node</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Node Repeat</th>\n",
       "      <th>Message Type</th>\n",
       "      <th>Component</th>\n",
       "      <th>Level</th>\n",
       "      <th>Content</th>\n",
       "      <th>Anomaly Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>1117838570</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03 15:42:50.675872</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>1117838573</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03 15:42:53.276129</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>1117838976</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03 15:49:36.156884</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>1117838978</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>2005-06-03 15:49:38.026704</td>\n",
       "      <td>R02-M1-N0-C:J12-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>1117842440</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R23-M0-NE-C:J05-U01</td>\n",
       "      <td>2005-06-03 16:47:20.730545</td>\n",
       "      <td>R23-M0-NE-C:J05-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>63543 double-hummer alignment exceptions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-</td>\n",
       "      <td>1117842974</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R24-M0-N1-C:J13-U11</td>\n",
       "      <td>2005-06-03 16:56:14.254137</td>\n",
       "      <td>R24-M0-N1-C:J13-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>162 double-hummer alignment exceptions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>1117843015</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R21-M1-N6-C:J08-U11</td>\n",
       "      <td>2005-06-03 16:56:55.309974</td>\n",
       "      <td>R21-M1-N6-C:J08-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>141 double-hummer alignment exceptions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "      <td>1117848119</td>\n",
       "      <td>2005.06.03</td>\n",
       "      <td>R16-M1-N2-C:J17-U01</td>\n",
       "      <td>2005-06-03 18:21:59.871925</td>\n",
       "      <td>R16-M1-N2-C:J17-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>CE sym 2, at 0x0b85eee0, mask 0x05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>APPREAD</td>\n",
       "      <td>1117869872</td>\n",
       "      <td>2005.06.04</td>\n",
       "      <td>R04-M1-N4-I:J18-U11</td>\n",
       "      <td>2005-06-04 00:24:32.432192</td>\n",
       "      <td>R04-M1-N4-I:J18-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>APP</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33569</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>APPREAD</td>\n",
       "      <td>1117869876</td>\n",
       "      <td>2005.06.04</td>\n",
       "      <td>R27-M1-N4-I:J18-U01</td>\n",
       "      <td>2005-06-04 00:24:36.222560</td>\n",
       "      <td>R27-M1-N4-I:J18-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>APP</td>\n",
       "      <td>FATAL</td>\n",
       "      <td>ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>1117942120</td>\n",
       "      <td>2005.06.04</td>\n",
       "      <td>R30-M0-N7-C:J08-U01</td>\n",
       "      <td>2005-06-04 20:28:40.767551</td>\n",
       "      <td>R30-M0-N7-C:J08-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>CE sym 20, at 0x1438f9e0, mask 0x40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "      <td>1117955341</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R25-M0-N7-C:J02-U01</td>\n",
       "      <td>2005-06-05 00:09:01.903373</td>\n",
       "      <td>R25-M0-N7-C:J02-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.2275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-</td>\n",
       "      <td>1117955392</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R24-M1-N8-C:J09-U11</td>\n",
       "      <td>2005-06-05 00:09:52.516674</td>\n",
       "      <td>R24-M1-N8-C:J09-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.862</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>1117956980</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R24-M1-NB-C:J15-U11</td>\n",
       "      <td>2005-06-05 00:36:20.945796</td>\n",
       "      <td>R24-M1-NB-C:J15-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.728</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-</td>\n",
       "      <td>1117957045</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R20-M1-N8-C:J04-U01</td>\n",
       "      <td>2005-06-05 00:37:25.012681</td>\n",
       "      <td>R20-M1-N8-C:J04-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-</td>\n",
       "      <td>1117959501</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R24-M0-NE-C:J14-U11</td>\n",
       "      <td>2005-06-05 01:18:21.778604</td>\n",
       "      <td>R24-M0-NE-C:J14-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.3276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "      <td>1117959513</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R21-M1-N2-C:J11-U01</td>\n",
       "      <td>2005-06-05 01:18:33.830595</td>\n",
       "      <td>R21-M1-N2-C:J11-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.1717</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-</td>\n",
       "      <td>1117959563</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R24-M0-N8-C:J04-U11</td>\n",
       "      <td>2005-06-05 01:19:23.822135</td>\n",
       "      <td>R24-M0-N8-C:J04-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.3919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-</td>\n",
       "      <td>1117973759</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R31-M0-NE-C:J05-U11</td>\n",
       "      <td>2005-06-05 05:15:59.416717</td>\n",
       "      <td>R31-M0-NE-C:J05-U11</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.2079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-</td>\n",
       "      <td>1117973786</td>\n",
       "      <td>2005.06.05</td>\n",
       "      <td>R36-M0-NA-C:J06-U01</td>\n",
       "      <td>2005-06-05 05:16:26.686603</td>\n",
       "      <td>R36-M0-NA-C:J06-U01</td>\n",
       "      <td>RAS</td>\n",
       "      <td>KERNEL</td>\n",
       "      <td>INFO</td>\n",
       "      <td>generating core.1414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Anomaly Type  Timestamp (ms)        Date                 Node  \\\n",
       "0             -      1117838570  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "1             -      1117838573  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "2             -      1117838976  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "3             -      1117838978  2005.06.03  R02-M1-N0-C:J12-U11   \n",
       "4             -      1117842440  2005.06.03  R23-M0-NE-C:J05-U01   \n",
       "5             -      1117842974  2005.06.03  R24-M0-N1-C:J13-U11   \n",
       "6             -      1117843015  2005.06.03  R21-M1-N6-C:J08-U11   \n",
       "7             -      1117848119  2005.06.03  R16-M1-N2-C:J17-U01   \n",
       "8       APPREAD      1117869872  2005.06.04  R04-M1-N4-I:J18-U11   \n",
       "9       APPREAD      1117869876  2005.06.04  R27-M1-N4-I:J18-U01   \n",
       "10            -      1117942120  2005.06.04  R30-M0-N7-C:J08-U01   \n",
       "11            -      1117955341  2005.06.05  R25-M0-N7-C:J02-U01   \n",
       "12            -      1117955392  2005.06.05  R24-M1-N8-C:J09-U11   \n",
       "13            -      1117956980  2005.06.05  R24-M1-NB-C:J15-U11   \n",
       "14            -      1117957045  2005.06.05  R20-M1-N8-C:J04-U01   \n",
       "15            -      1117959501  2005.06.05  R24-M0-NE-C:J14-U11   \n",
       "16            -      1117959513  2005.06.05  R21-M1-N2-C:J11-U01   \n",
       "17            -      1117959563  2005.06.05  R24-M0-N8-C:J04-U11   \n",
       "18            -      1117973759  2005.06.05  R31-M0-NE-C:J05-U11   \n",
       "19            -      1117973786  2005.06.05  R36-M0-NA-C:J06-U01   \n",
       "\n",
       "                    Timestamp          Node Repeat Message Type Component  \\\n",
       "0  2005-06-03 15:42:50.675872  R02-M1-N0-C:J12-U11          RAS    KERNEL   \n",
       "1  2005-06-03 15:42:53.276129  R02-M1-N0-C:J12-U11          RAS    KERNEL   \n",
       "2  2005-06-03 15:49:36.156884  R02-M1-N0-C:J12-U11          RAS    KERNEL   \n",
       "3  2005-06-03 15:49:38.026704  R02-M1-N0-C:J12-U11          RAS    KERNEL   \n",
       "4  2005-06-03 16:47:20.730545  R23-M0-NE-C:J05-U01          RAS    KERNEL   \n",
       "5  2005-06-03 16:56:14.254137  R24-M0-N1-C:J13-U11          RAS    KERNEL   \n",
       "6  2005-06-03 16:56:55.309974  R21-M1-N6-C:J08-U11          RAS    KERNEL   \n",
       "7  2005-06-03 18:21:59.871925  R16-M1-N2-C:J17-U01          RAS    KERNEL   \n",
       "8  2005-06-04 00:24:32.432192  R04-M1-N4-I:J18-U11          RAS       APP   \n",
       "9  2005-06-04 00:24:36.222560  R27-M1-N4-I:J18-U01          RAS       APP   \n",
       "10 2005-06-04 20:28:40.767551  R30-M0-N7-C:J08-U01          RAS    KERNEL   \n",
       "11 2005-06-05 00:09:01.903373  R25-M0-N7-C:J02-U01          RAS    KERNEL   \n",
       "12 2005-06-05 00:09:52.516674  R24-M1-N8-C:J09-U11          RAS    KERNEL   \n",
       "13 2005-06-05 00:36:20.945796  R24-M1-NB-C:J15-U11          RAS    KERNEL   \n",
       "14 2005-06-05 00:37:25.012681  R20-M1-N8-C:J04-U01          RAS    KERNEL   \n",
       "15 2005-06-05 01:18:21.778604  R24-M0-NE-C:J14-U11          RAS    KERNEL   \n",
       "16 2005-06-05 01:18:33.830595  R21-M1-N2-C:J11-U01          RAS    KERNEL   \n",
       "17 2005-06-05 01:19:23.822135  R24-M0-N8-C:J04-U11          RAS    KERNEL   \n",
       "18 2005-06-05 05:15:59.416717  R31-M0-NE-C:J05-U11          RAS    KERNEL   \n",
       "19 2005-06-05 05:16:26.686603  R36-M0-NA-C:J06-U01          RAS    KERNEL   \n",
       "\n",
       "    Level  \\\n",
       "0    INFO   \n",
       "1    INFO   \n",
       "2    INFO   \n",
       "3    INFO   \n",
       "4    INFO   \n",
       "5    INFO   \n",
       "6    INFO   \n",
       "7    INFO   \n",
       "8   FATAL   \n",
       "9   FATAL   \n",
       "10   INFO   \n",
       "11   INFO   \n",
       "12   INFO   \n",
       "13   INFO   \n",
       "14   INFO   \n",
       "15   INFO   \n",
       "16   INFO   \n",
       "17   INFO   \n",
       "18   INFO   \n",
       "19   INFO   \n",
       "\n",
       "                                                                                           Content  \\\n",
       "0                                                         instruction cache parity error corrected   \n",
       "1                                                         instruction cache parity error corrected   \n",
       "2                                                         instruction cache parity error corrected   \n",
       "3                                                         instruction cache parity error corrected   \n",
       "4                                                         63543 double-hummer alignment exceptions   \n",
       "5                                                           162 double-hummer alignment exceptions   \n",
       "6                                                           141 double-hummer alignment exceptions   \n",
       "7                                                               CE sym 2, at 0x0b85eee0, mask 0x05   \n",
       "8   ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33569   \n",
       "9   ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33370   \n",
       "10                                                             CE sym 20, at 0x1438f9e0, mask 0x40   \n",
       "11                                                                            generating core.2275   \n",
       "12                                                                             generating core.862   \n",
       "13                                                                             generating core.728   \n",
       "14                                                                             generating core.775   \n",
       "15                                                                            generating core.3276   \n",
       "16                                                                            generating core.1717   \n",
       "17                                                                            generating core.3919   \n",
       "18                                                                            generating core.2079   \n",
       "19                                                                            generating core.1414   \n",
       "\n",
       "    Anomaly Label  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "5               0  \n",
       "6               0  \n",
       "7               0  \n",
       "8               1  \n",
       "9               1  \n",
       "10              0  \n",
       "11              0  \n",
       "12              0  \n",
       "13              0  \n",
       "14              0  \n",
       "15              0  \n",
       "16              0  \n",
       "17              0  \n",
       "18              0  \n",
       "19              0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df = pd.read_csv(log_csv_file, names=['Anomaly Type', 'Timestamp (ms)', 'Date', 'Node', 'Timestamp', 'Node Repeat', 'Message Type', 'Component', 'Level', 'Content'])\n",
    "original_df['Anomaly Label'] = np.where(original_df['Anomaly Type'] == '-', 0, 1)\n",
    "original_df['Timestamp'] = pd.to_datetime(original_df['Timestamp'], format='%Y-%m-%d-%H.%M.%S.%f')\n",
    "\n",
    "original_df.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15   2005-06-05 01:18:21.778604\n",
       "16   2005-06-05 01:18:33.830595\n",
       "17   2005-06-05 01:19:23.822135\n",
       "18   2005-06-05 05:15:59.416717\n",
       "19   2005-06-05 05:16:26.686603\n",
       "20   2005-06-05 05:18:39.396608\n",
       "21   2005-06-05 05:23:26.239153\n",
       "22   2005-06-05 05:27:43.336565\n",
       "23   2005-06-05 05:40:51.726735\n",
       "24   2005-06-05 06:04:18.406158\n",
       "25   2005-06-05 06:18:17.802159\n",
       "26   2005-06-05 06:47:07.157021\n",
       "27   2005-06-05 07:43:29.979844\n",
       "28   2005-06-05 08:08:44.281729\n",
       "29   2005-06-05 08:08:50.547117\n",
       "30   2005-06-05 08:10:16.270131\n",
       "31   2005-06-05 08:10:46.344235\n",
       "32   2005-06-05 08:30:01.873693\n",
       "33   2005-06-05 08:30:13.824307\n",
       "34   2005-06-05 08:31:04.464776\n",
       "35   2005-06-05 08:32:13.659715\n",
       "36   2005-06-05 08:32:27.814949\n",
       "37   2005-06-05 09:03:40.673488\n",
       "38   2005-06-05 09:17:46.225683\n",
       "39   2005-06-05 09:18:06.694851\n",
       "40   2005-06-05 09:20:16.681318\n",
       "41   2005-06-05 09:20:43.944594\n",
       "42   2005-06-05 09:38:03.456120\n",
       "43   2005-06-05 09:38:28.957918\n",
       "44   2005-06-05 09:38:50.430385\n",
       "45   2005-06-05 09:39:37.590924\n",
       "46   2005-06-05 09:39:54.210760\n",
       "47   2005-06-05 09:40:01.191177\n",
       "48   2005-06-05 09:40:17.352982\n",
       "Name: Timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = original_df.loc[(original_df['Timestamp'] >= '2005-06-05 01:00:00.000000') & (original_df['Timestamp'] < '2005-06-05 10:00:00.000000')]\n",
    "filtered_df['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anomaly Label</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>instruction cache parity error corrected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>63543 double-hummer alignment exceptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>162 double-hummer alignment exceptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>141 double-hummer alignment exceptions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>CE sym 2, at 0x0b85eee0, mask 0x05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>CE sym 20, at 0x1438f9e0, mask 0x40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.2275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.3276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.1717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.3919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.2079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>generating core.1414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Anomaly Label  \\\n",
       "0               0   \n",
       "1               0   \n",
       "2               0   \n",
       "3               0   \n",
       "4               0   \n",
       "5               0   \n",
       "6               0   \n",
       "7               0   \n",
       "8               1   \n",
       "9               1   \n",
       "10              0   \n",
       "11              0   \n",
       "12              0   \n",
       "13              0   \n",
       "14              0   \n",
       "15              0   \n",
       "16              0   \n",
       "17              0   \n",
       "18              0   \n",
       "19              0   \n",
       "\n",
       "                                                                                           Content  \n",
       "0                                                         instruction cache parity error corrected  \n",
       "1                                                         instruction cache parity error corrected  \n",
       "2                                                         instruction cache parity error corrected  \n",
       "3                                                         instruction cache parity error corrected  \n",
       "4                                                         63543 double-hummer alignment exceptions  \n",
       "5                                                           162 double-hummer alignment exceptions  \n",
       "6                                                           141 double-hummer alignment exceptions  \n",
       "7                                                               CE sym 2, at 0x0b85eee0, mask 0x05  \n",
       "8   ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33569  \n",
       "9   ciod: failed to read message prefix on control stream (CioStream socket to 172.16.96.116:33370  \n",
       "10                                                             CE sym 20, at 0x1438f9e0, mask 0x40  \n",
       "11                                                                            generating core.2275  \n",
       "12                                                                             generating core.862  \n",
       "13                                                                             generating core.728  \n",
       "14                                                                             generating core.775  \n",
       "15                                                                            generating core.3276  \n",
       "16                                                                            generating core.1717  \n",
       "17                                                                            generating core.3919  \n",
       "18                                                                            generating core.2079  \n",
       "19                                                                            generating core.1414  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = original_df[['Anomaly Label', 'Content']]\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Log Sequences and Labels for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = math.floor(df['Content'].index.size/WINDOW_SIZE)\n",
    "r = math.floor(df['Content'].index.size%WINDOW_SIZE)\n",
    "\n",
    "if r != 0:\n",
    "    log_seqs = np.array(np.split(np.array(df['Content'])[:-r], n))\n",
    "else:\n",
    "    log_seqs = np.array(np.split(np.array(df['Content']), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if r != 0:\n",
    "    log_seq_idx = np.array(np.split(df.index.to_numpy()[:-r], n))\n",
    "else:\n",
    "    log_seq_idx = np.array(np.split(df.index.to_numpy(), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_seq_anomaly_labels = np.empty([n], dtype=int)\n",
    "i = 0\n",
    "for seq in log_seq_idx:\n",
    "    if np.sum(df.loc[seq]['Anomaly Label'].values) > 0:\n",
    "        log_seq_anomaly_labels[i] = 1\n",
    "    else:\n",
    "        log_seq_anomaly_labels[i] = 0\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Parsing & Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drain Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(drain_input_dir + '/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "files = glob.glob(drain_output_dir + '/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "drain_config_file = path.abspath(path.join(project_root, config_dir, 'drain3.ini'))\n",
    "\n",
    "main_structured_csv_filename = 'BGL_main_structured.csv'\n",
    "drain_main_structured_csv_file = path.abspath(path.join(project_root, drain_output_dir, main_structured_csv_filename))\n",
    "\n",
    "templates_csv_filename = 'BGL_main_templates.csv'\n",
    "drain_templates_csv_file = path.abspath(path.join(project_root, drain_output_dir, templates_csv_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrainParser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        config = TemplateMinerConfig()\n",
    "        config.load(drain_config_file)\n",
    "\n",
    "        self.template_miner = TemplateMiner(config=config)\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, log_seqs, y = None):\n",
    "        log_seqs_list = log_seqs.reshape([-1]).tolist()\n",
    "        self.parsed = []\n",
    "\n",
    "        for line in log_seqs_list:\n",
    "            self.parsed.append(self.template_miner.add_log_message(line))\n",
    "\n",
    "        # Uncomment during debug to view the parser output\n",
    "        # self.write_output_to_csv()\n",
    "\n",
    "        template_seq = [x['cluster_id']-1 for x in self.parsed]\n",
    "        n = math.floor(len(template_seq)/WINDOW_SIZE)\n",
    "        template_seqs = np.array(np.split(np.array(template_seq), n))\n",
    "\n",
    "        return template_seqs\n",
    "    \n",
    "    def cluster_template_to_tuple(self, cluster):\n",
    "        return (cluster.cluster_id, cluster.get_template(), cluster.size,)\n",
    "\n",
    "    def write_output_to_csv(self):\n",
    "        with open(drain_main_structured_csv_file, 'w') as drain_main_structured_csv_file_obj:\n",
    "            main_structured_csv_filewriter = DictWriter(drain_main_structured_csv_file_obj, fieldnames=['template_mined', 'cluster_id', 'change_type', 'cluster_size', 'cluster_count'])\n",
    "            main_structured_csv_filewriter.writeheader()\n",
    "            for line in self.parsed:\n",
    "                main_structured_csv_filewriter.writerow(line)\n",
    "            drain_main_structured_csv_file_obj.close\n",
    "            \n",
    "        clusters = self.template_miner.drain.clusters\n",
    "\n",
    "        with open(drain_templates_csv_file, 'a') as drain_templates_csv_file_obj:\n",
    "            drain_templates_csv_filewriter = writer(drain_templates_csv_file_obj)\n",
    "            drain_templates_csv_filewriter.writerow(header for header in ['cluster_id', 'template', 'size'])\n",
    "            for line in clusters:\n",
    "                drain_templates_csv_filewriter.writerow(self.cluster_template_to_tuple(line))\n",
    "            drain_templates_csv_file_obj.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spell Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_content_csv_file_name = 'bgl_2k_content.csv'\n",
    "log_content_csv_file = log_file = path.abspath(path.join(project_root, spell_input_dir, log_content_csv_file_name))\n",
    "\n",
    "main_structured_csv_filename = 'BGL_main_structured.csv'\n",
    "spell_main_structured_csv_file = path.abspath(path.join(project_root, spell_output_dir, main_structured_csv_filename))\n",
    "\n",
    "templates_csv_filename = 'BGL_main_templates.csv'\n",
    "spell_templates_csv_file = path.abspath(path.join(project_root, spell_output_dir, templates_csv_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellParser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        log_format = '<Content>'\n",
    "        tau = 0.5\n",
    "\n",
    "        self.parser = spell.LogParser(indir=spell_input_dir, outdir=spell_output_dir,\n",
    "                             log_format=log_format, tau=tau, logmain='BGL')\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, log_seqs, y = None):\n",
    "        log_seqs_list = log_seqs.reshape([-1]).tolist()\n",
    "\n",
    "        ldf = pd.DataFrame(log_seqs_list, columns=['Content'])\n",
    "        ldf.to_csv(log_content_csv_file, index=False, header=False)\n",
    "\n",
    "        self.parser.parse(log_content_csv_file_name)\n",
    "\n",
    "        nums = self.numericalize()\n",
    "        n = math.floor(len(nums)/WINDOW_SIZE)\n",
    "        nums = np.array(np.split(nums, n))\n",
    "        \n",
    "        # Comment during debug to view parser output\n",
    "        self.cleanup_files()\n",
    "        \n",
    "        return nums\n",
    "\n",
    "    def numericalize(self):\n",
    "        output_df = pd.read_csv(spell_main_structured_csv_file)\n",
    "\n",
    "        return output_df['EventId'].to_numpy()\n",
    "\n",
    "    def cleanup_files(self):\n",
    "        files = glob.glob(spell_input_dir + '/*')\n",
    "        for f in files:\n",
    "            os.remove(f)\n",
    "\n",
    "        files = glob.glob(spell_output_dir + '/*')\n",
    "        for f in files:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.NUMBER_OF_DIMENSIONS = 100\n",
    "\n",
    "    def fit(self, template_seqs, y = None):\n",
    "        template_seqs_filename = 'bgl_train_seqs.txt'\n",
    "        template_seqs_file = path.abspath(path.join(project_root, spell_output_dir, template_seqs_filename))\n",
    "        np.savetxt(template_seqs_file, template_seqs, fmt='%s')\n",
    "        self.fasttext_model = fasttext.train_unsupervised(template_seqs_file, model='cbow', minCount=1, dim=self.NUMBER_OF_DIMENSIONS)\n",
    "        \n",
    "        # Comment during debug to view embedding input\n",
    "        os.remove(template_seqs_file)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, template_seqs, y = None):\n",
    "        template_seqs_ = template_seqs.copy()\n",
    "        template_seqs_ = np.apply_along_axis(self.average_embeddings, 1, template_seqs)\n",
    "\n",
    "        return template_seqs_\n",
    "\n",
    "    def average_embeddings(self, num_lse_vector):\n",
    "        s2v_vector = self.fasttext_model.get_sentence_vector(' '.join(np.vectorize(str)(num_lse_vector)))\n",
    "        return s2v_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-11-25 15:17:22,130][INFO]: Starting Drain3 template miner\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266,) (134,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  87\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7252 lr:  0.000000 avg.loss:  4.131035 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:22,883][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:22,896][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:22,899][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:22,992][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:23,405][INFO]: Output parse file\n",
      "[2022-11-25 15:17:23,414][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:23,426][INFO]: lastestLindId: 1330\n",
      "[2022-11-25 15:17:23,450][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:23,452][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:23,453][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:23,454][INFO]: Parsing done. [Time taken: 0:00:00.570817]\n",
      "Read 0M words\n",
      "Number of words:  69\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  675231 lr:  0.000000 avg.loss:  4.137098 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:24,327][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:24,334][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:24,337][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:24,394][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:24,418][INFO]: Output parse file\n",
      "[2022-11-25 15:17:24,424][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:24,433][INFO]: lastestLindId: 670\n",
      "[2022-11-25 15:17:24,448][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:24,450][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:24,452][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:24,452][INFO]: Parsing done. [Time taken: 0:00:00.125368]\n",
      "[2022-11-25 15:17:24,468][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:24,476][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:24,479][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:24,534][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:24,559][INFO]: Output parse file\n",
      "[2022-11-25 15:17:24,565][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:24,573][INFO]: lastestLindId: 670\n",
      "[2022-11-25 15:17:24,587][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:24,589][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:24,590][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:24,592][INFO]: Parsing done. [Time taken: 0:00:00.123160]\n",
      "[2022-11-25 15:17:24,612][INFO]: Starting Drain3 template miner\n",
      "Read 0M words\n",
      "Number of words:  90\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267,) (133,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:    7271 lr:  0.000000 avg.loss:  4.134593 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:25,229][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:25,242][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:25,245][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:25,343][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:25,391][INFO]: Output parse file\n",
      "[2022-11-25 15:17:25,401][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:25,415][INFO]: lastestLindId: 1335\n",
      "[2022-11-25 15:17:25,444][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:25,447][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:25,448][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:25,449][INFO]: Parsing done. [Time taken: 0:00:00.220105]\n",
      "Read 0M words\n",
      "Number of words:  73\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7419 lr:  0.000000 avg.loss:  4.136079 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:26,034][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:26,044][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:26,046][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:26,099][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:26,275][INFO]: Output parse file\n",
      "[2022-11-25 15:17:26,281][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:26,288][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:26,304][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:26,307][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:26,309][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:26,309][INFO]: Parsing done. [Time taken: 0:00:00.275383]\n",
      "[2022-11-25 15:17:26,327][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:26,336][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:26,339][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:26,395][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:26,565][INFO]: Output parse file\n",
      "[2022-11-25 15:17:26,570][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:26,580][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:26,595][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:26,598][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:26,599][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:26,600][INFO]: Parsing done. [Time taken: 0:00:00.273158]\n",
      "[2022-11-25 15:17:26,621][INFO]: Starting Drain3 template miner\n",
      "Read 0M words\n",
      "Number of words:  93\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267,) (133,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  983113 lr:  0.000000 avg.loss:  4.128119 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:27,159][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:27,177][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:27,180][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:27,284][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:27,433][INFO]: Output parse file\n",
      "[2022-11-25 15:17:27,442][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:27,458][INFO]: lastestLindId: 1335\n",
      "[2022-11-25 15:17:27,485][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:27,487][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:27,489][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:27,490][INFO]: Parsing done. [Time taken: 0:00:00.331063]\n",
      "Read 0M words\n",
      "Number of words:  78\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7135 lr:  0.000000 avg.loss:  4.141453 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:28,075][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:28,087][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:28,090][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:28,145][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:28,171][INFO]: Output parse file\n",
      "[2022-11-25 15:17:28,177][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:28,187][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:28,204][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:28,206][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:28,208][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:28,209][INFO]: Parsing done. [Time taken: 0:00:00.134368]\n",
      "[2022-11-25 15:17:28,223][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:28,231][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:28,234][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:28,282][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:28,309][INFO]: Output parse file\n",
      "[2022-11-25 15:17:28,315][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:28,322][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:28,339][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:28,342][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:28,344][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:28,345][INFO]: Parsing done. [Time taken: 0:00:00.121241]\n",
      "[2022-11-25 15:17:28,367][INFO]: Starting Drain3 template miner\n",
      "Read 0M words\n",
      "Number of words:  93\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266,) (134,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread: 1276777 lr:  0.000000 avg.loss:  4.131452 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:28,919][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:28,932][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:28,934][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:29,033][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:29,296][INFO]: Output parse file\n",
      "[2022-11-25 15:17:29,304][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:29,318][INFO]: lastestLindId: 1330\n",
      "[2022-11-25 15:17:29,346][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:29,349][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:29,351][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:29,352][INFO]: Parsing done. [Time taken: 0:00:00.432603]\n",
      "Read 0M words\n",
      "Number of words:  77\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7516 lr:  0.000000 avg.loss:  4.135534 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:29,941][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:29,948][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:29,950][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:30,000][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:30,028][INFO]: Output parse file\n",
      "[2022-11-25 15:17:30,033][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:30,041][INFO]: lastestLindId: 670\n",
      "[2022-11-25 15:17:30,055][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:30,058][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:30,060][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:30,061][INFO]: Parsing done. [Time taken: 0:00:00.120119]\n",
      "[2022-11-25 15:17:30,077][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:30,086][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:30,088][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:30,141][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:30,169][INFO]: Output parse file\n",
      "[2022-11-25 15:17:30,174][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:30,187][INFO]: lastestLindId: 670\n",
      "[2022-11-25 15:17:30,204][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:30,206][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:30,208][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:30,209][INFO]: Parsing done. [Time taken: 0:00:00.131349]\n",
      "[2022-11-25 15:17:30,235][INFO]: Starting Drain3 template miner\n",
      "Read 0M words\n",
      "Number of words:  92\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267,) (133,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:    7369 lr:  0.000000 avg.loss:  4.140278 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:30,872][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:30,897][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:30,900][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:31,015][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:31,169][INFO]: Output parse file\n",
      "[2022-11-25 15:17:31,178][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:31,190][INFO]: lastestLindId: 1335\n",
      "[2022-11-25 15:17:31,216][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:31,219][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:31,220][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:31,220][INFO]: Parsing done. [Time taken: 0:00:00.348666]\n",
      "Read 0M words\n",
      "Number of words:  76\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7369 lr:  0.000000 avg.loss:  4.133438 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:31,892][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:31,899][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:31,902][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:31,962][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:32,098][INFO]: Output parse file\n",
      "[2022-11-25 15:17:32,103][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:32,112][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:32,137][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:32,140][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:32,142][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:32,144][INFO]: Parsing done. [Time taken: 0:00:00.252096]\n",
      "[2022-11-25 15:17:32,160][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:32,168][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:32,171][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:32,229][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:32,373][INFO]: Output parse file\n",
      "[2022-11-25 15:17:32,381][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:32,393][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:32,410][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:32,413][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:32,415][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:32,416][INFO]: Parsing done. [Time taken: 0:00:00.256541]\n",
      "[2022-11-25 15:17:32,442][INFO]: Starting Drain3 template miner\n",
      "Read 0M words\n",
      "Number of words:  91\n",
      "Number of labels: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267,) (133,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:    7473 lr:  0.000000 avg.loss:  4.136121 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:33,221][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:33,235][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:33,237][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:33,332][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:33,394][INFO]: Output parse file\n",
      "[2022-11-25 15:17:33,404][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:33,417][INFO]: lastestLindId: 1335\n",
      "[2022-11-25 15:17:33,437][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:33,439][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:33,441][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:33,442][INFO]: Parsing done. [Time taken: 0:00:00.221012]\n",
      "Read 0M words\n",
      "Number of words:  70\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:    7248 lr:  0.000000 avg.loss:  4.135736 ETA:   0h 0m 0s\n",
      "[2022-11-25 15:17:34,041][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:34,050][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:34,052][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:34,107][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:34,136][INFO]: Output parse file\n",
      "[2022-11-25 15:17:34,143][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:34,152][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:34,169][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:34,172][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:34,173][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:34,174][INFO]: Parsing done. [Time taken: 0:00:00.133325]\n",
      "[2022-11-25 15:17:34,190][INFO]: Parsing file: input/spell/bgl_2k_content.csv\n",
      "[2022-11-25 15:17:34,199][INFO]: Loaded 100.0% of log lines.\n",
      "[2022-11-25 15:17:34,201][INFO]: load_data() finished!\n",
      "[2022-11-25 15:17:34,259][INFO]: Processed 100.0% of log lines.\n",
      "[2022-11-25 15:17:34,286][INFO]: Output parse file\n",
      "[2022-11-25 15:17:34,292][INFO]: Output main file for append\n",
      "[2022-11-25 15:17:34,302][INFO]: lastestLindId: 665\n",
      "[2022-11-25 15:17:34,317][INFO]: rootNodePath: output/spell/rootNode.pkl\n",
      "[2022-11-25 15:17:34,319][INFO]: logCluLPath: output/spell/logCluL.pkl\n",
      "[2022-11-25 15:17:34,321][INFO]: Store objects done.\n",
      "[2022-11-25 15:17:34,321][INFO]: Parsing done. [Time taken: 0:00:00.131737]\n"
     ]
    }
   ],
   "source": [
    "drain_y_real = []\n",
    "drain_y_proba = []\n",
    "\n",
    "drain_precisions = []\n",
    "drain_recalls = []\n",
    "drain_avg_precisions = []\n",
    "\n",
    "drain_tprs = []\n",
    "drain_tprs2 = []\n",
    "drain_fprs = []\n",
    "drain_aucs = []\n",
    "drain_mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "drain_f1_scores = []\n",
    "drain_accuracy_scores = []\n",
    "drain_precision_scores = []\n",
    "drain_recall_scores = []\n",
    "\n",
    "drain_index = 0\n",
    "\n",
    "spell_y_real = []\n",
    "spell_y_proba = []\n",
    "\n",
    "spell_precisions = []\n",
    "spell_recalls = []\n",
    "spell_avg_precisions = []\n",
    "\n",
    "spell_tprs = []\n",
    "spell_tprs2 = []\n",
    "spell_fprs = []\n",
    "spell_aucs = []\n",
    "spell_mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "spell_f1_scores = []\n",
    "spell_accuracy_scores = []\n",
    "spell_precision_scores = []\n",
    "spell_recall_scores = []\n",
    "\n",
    "spell_index = 0\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=0)\n",
    "for train, test in cv.split(log_seqs, log_seq_anomaly_labels):\n",
    "    print(train.shape, test.shape)\n",
    "\n",
    "    drain_pipe = Pipeline(steps=[('parsing', DrainParser()), ('word_embedding', WordEmbedding()), ('gnb', GaussianNB())])\n",
    "    drain_probas_ = drain_pipe.fit(log_seqs[train], log_seq_anomaly_labels[train]).predict_proba(log_seqs[test])\n",
    "    drain_y_pred = drain_pipe.predict(log_seqs[test])\n",
    "\n",
    "    spell_pipe = Pipeline(steps=[('parsing', SpellParser()), ('word_embedding', WordEmbedding()), ('gnb', GaussianNB())])\n",
    "    spell_probas_ = spell_pipe.fit(log_seqs[train], log_seq_anomaly_labels[train]).predict_proba(log_seqs[test])\n",
    "    spell_y_pred = spell_pipe.predict(log_seqs[test])\n",
    "\n",
    "    ####### PR #######\n",
    "\n",
    "    drain_precision, drain_recall, _ = precision_recall_curve(log_seq_anomaly_labels[test], drain_probas_[:, 1])\n",
    "    drain_precisions.append(drain_precision)\n",
    "    drain_recalls.append(drain_recall)\n",
    "\n",
    "    drain_avg_precision = average_precision_score(log_seq_anomaly_labels[test], drain_probas_[:, 1])\n",
    "    drain_avg_precisions.append(drain_avg_precision)\n",
    "        \n",
    "    drain_y_real.append(log_seq_anomaly_labels[test])\n",
    "    drain_y_proba.append(drain_probas_[:, 1])\n",
    "\n",
    "    ####### ROC #######\n",
    "\n",
    "    drain_fpr, drain_tpr, _ = roc_curve(log_seq_anomaly_labels[test], drain_probas_[:, 1])\n",
    "    drain_tprs.append(np.interp(drain_mean_fpr, drain_fpr, drain_tpr))\n",
    "    drain_tprs2.append(drain_tpr)\n",
    "    drain_fprs.append(drain_fpr)\n",
    "        \n",
    "    drain_tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(drain_fpr, drain_tpr)\n",
    "    drain_aucs.append(roc_auc)\n",
    "\n",
    "    ####### F1-score, Accuracy, Precision and Recall #######\n",
    "\n",
    "    drain_f1_scores.append(f1_score(log_seq_anomaly_labels[test], drain_y_pred))\n",
    "    drain_accuracy_scores.append(accuracy_score(log_seq_anomaly_labels[test], drain_y_pred))\n",
    "    drain_precision_scores.append(precision_score(log_seq_anomaly_labels[test], drain_y_pred))\n",
    "    drain_recall_scores.append(recall_score(log_seq_anomaly_labels[test], drain_y_pred))\n",
    "\n",
    "    drain_index += 1\n",
    "\n",
    "    ####### PR #######\n",
    "\n",
    "    spell_precision, spell_recall, _ = precision_recall_curve(log_seq_anomaly_labels[test], spell_probas_[:, 1])\n",
    "    spell_precisions.append(spell_precision)\n",
    "    spell_recalls.append(spell_recall)\n",
    "\n",
    "    spell_avg_precision = average_precision_score(log_seq_anomaly_labels[test], spell_probas_[:, 1])\n",
    "    spell_avg_precisions.append(spell_avg_precision)\n",
    "        \n",
    "    spell_y_real.append(log_seq_anomaly_labels[test])\n",
    "    spell_y_proba.append(spell_probas_[:, 1])\n",
    "\n",
    "    ####### ROC #######\n",
    "\n",
    "    spell_fpr, spell_tpr, _ = roc_curve(log_seq_anomaly_labels[test], spell_probas_[:, 1])\n",
    "    spell_tprs.append(np.interp(spell_mean_fpr, spell_fpr, spell_tpr))\n",
    "    spell_tprs2.append(spell_tpr)\n",
    "    spell_fprs.append(spell_fpr)\n",
    "        \n",
    "    spell_tprs[-1][0] = 0.0\n",
    "    roc_auc = auc(spell_fpr, spell_tpr)\n",
    "    spell_aucs.append(roc_auc)\n",
    "\n",
    "    ####### F1-score, Accuracy, Precision and Recall #######\n",
    "\n",
    "    spell_f1_scores.append(f1_score(log_seq_anomaly_labels[test], spell_y_pred))\n",
    "    spell_accuracy_scores.append(accuracy_score(log_seq_anomaly_labels[test], spell_y_pred))\n",
    "    spell_precision_scores.append(precision_score(log_seq_anomaly_labels[test], spell_y_pred))\n",
    "    spell_recall_scores.append(recall_score(log_seq_anomaly_labels[test], spell_y_pred))\n",
    "\n",
    "    spell_index += 1\n",
    "\n",
    "drain_y_real = np.concatenate(drain_y_real)\n",
    "drain_y_proba = np.concatenate(drain_y_proba)\n",
    "\n",
    "spell_y_real = np.concatenate(spell_y_real)\n",
    "spell_y_proba = np.concatenate(spell_y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores\n",
      "\n",
      "\n",
      "Spell:  [0.6341463414634146, 0.55, 0.6046511627906976, 0.6666666666666667, 0.6222222222222222, 0.4827586206896552]\n",
      "\n",
      "\n",
      "Drain:  [0.6666666666666667, 0.5925925925925927, 0.6511627906976745, 0.6206896551724138, 0.6153846153846154, 0.631578947368421]\n",
      "\n",
      "\n",
      "Average Scores\n",
      "Spell:  0.5934075023054427  Drain:  0.6296792113137307\n",
      "\n",
      "\n",
      "Accuracy Scores\n",
      "\n",
      "\n",
      "Spell:  [0.8880597014925373, 0.8646616541353384, 0.8721804511278195, 0.8805970149253731, 0.8721804511278195, 0.8872180451127819]\n",
      "\n",
      "\n",
      "Drain:  [0.8805970149253731, 0.8345864661654135, 0.8872180451127819, 0.835820895522388, 0.849624060150376, 0.8947368421052632]\n",
      "\n",
      "\n",
      "Average Scores\n",
      "Spell:  0.8774828863202783  Drain:  0.8637638873302659\n",
      "\n",
      "\n",
      "Precision Scores\n",
      "\n",
      "\n",
      "Spell:  [0.5652173913043478, 0.4782608695652174, 0.52, 0.5333333333333333, 0.5, 0.6363636363636364]\n",
      "\n",
      "\n",
      "Drain:  [0.5333333333333333, 0.43243243243243246, 0.56, 0.45, 0.45714285714285713, 0.6]\n",
      "\n",
      "\n",
      "Spell:  0.5388625384277558  Drain:  0.5054847704847705\n",
      "\n",
      "\n",
      "Recall Scores\n",
      "\n",
      "\n",
      "Spell:  [0.7222222222222222, 0.6470588235294118, 0.7222222222222222, 0.8888888888888888, 0.8235294117647058, 0.3888888888888889]\n",
      "\n",
      "\n",
      "Drain:  [0.8888888888888888, 0.9411764705882353, 0.7777777777777778, 1.0, 0.9411764705882353, 0.6666666666666666]\n",
      "\n",
      "\n",
      "Spell:  0.69880174291939  Drain:  0.869281045751634\n"
     ]
    }
   ],
   "source": [
    "print('F1 Scores')\n",
    "print('\\n')\n",
    "print('Spell: ', spell_f1_scores)\n",
    "print('\\n')\n",
    "print('Drain: ',drain_f1_scores)\n",
    "print('\\n')\n",
    "print('Average Scores')\n",
    "print('Spell: ', np.average(spell_f1_scores), ' Drain: ', np.average(drain_f1_scores))\n",
    "\n",
    "print('\\n')\n",
    "print('Accuracy Scores')\n",
    "print('\\n')\n",
    "print('Spell: ', spell_accuracy_scores)\n",
    "print('\\n')\n",
    "print('Drain: ', drain_accuracy_scores)\n",
    "print('\\n')\n",
    "print('Average Scores')\n",
    "print('Spell: ', np.average(spell_accuracy_scores), ' Drain: ', np.average(drain_accuracy_scores))\n",
    "\n",
    "print('\\n')\n",
    "print('Precision Scores')\n",
    "print('\\n')\n",
    "print('Spell: ', spell_precision_scores)\n",
    "print('\\n')\n",
    "print('Drain: ', drain_precision_scores)\n",
    "print('\\n')\n",
    "print('Spell: ', np.average(spell_precision_scores), ' Drain: ', np.average(drain_precision_scores))\n",
    "\n",
    "print('\\n')\n",
    "print('Recall Scores')\n",
    "print('\\n')\n",
    "print('Spell: ', spell_recall_scores)\n",
    "print('\\n')\n",
    "print('Drain: ', drain_recall_scores)\n",
    "print('\\n')\n",
    "print('Spell: ', np.average(spell_recall_scores), ' Drain: ', np.average(drain_recall_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('ms-ds13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b7d5d95ec871826f4c96f39204edc263930b1065d3d6b0d23c0891d0cf865f41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
